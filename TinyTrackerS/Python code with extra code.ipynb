{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba3c4b-eef2-410f-90a2-d1dc87d33989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to run the model (pytorch)\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "checkpoint = torch.load(\"yolov8n-seg.pt\", map_location=torch.device(\"cpu\"), weights_only=False)\n",
    "print(checkpoint.keys())\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "results = model(\"download.jpg\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e76449-05f7-480f-930b-2044ee6bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit the model to add the layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "pytorch_model = model.model\n",
    "pytorch_model.new_layer = nn.Linear(100, 10)\n",
    "torch.save(pytorch_model.state_dict(), \"modified_model.pt\")\n",
    "print(\"Modified model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c8aec-5fed-4900-b0ca-8d31e7ae9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "pytorch_model = model.model\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970273d1-ad61-40ba-b36e-bc5c0fd10d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "interpreter = tf.lite.Interpreter(model_path=\"yolov8-face.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Input Details:\", input_details)\n",
    "print(\"Output Details:\", output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500806b-4446-4ce2-b667-58578a90cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run face detection model (pytorch)\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8-face.pt\")\n",
    "results = model(\"mon.jpg\")\n",
    "results[0].show()\n",
    "\n",
    "results[0].save(\"resultpytroch.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075436c5-c0d8-44eb-864f-a0fdca730f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8-face.pt\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "\n",
    "        results = model(frame)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"YOLOv8 Face Detection\", annotated_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or cv2.getWindowProperty(\"YOLOv8 Face Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299fb4d-22df-4ce0-9b6a-89d2fe0aa596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run face detection model (tflite)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "image_path = \"Picture1.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_data = np.expand_dims(image.resize((640, 640)), axis=0) / 255.0\n",
    "input_data = input_data.astype(np.float32)\n",
    "\n",
    "model_path = \"yolov8-face.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "outputs = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "detections = np.squeeze(outputs)\n",
    "boxes = detections[:4, :]\n",
    "scores = detections[4, :]\n",
    "\n",
    "x_center, y_center, width, height = boxes\n",
    "x1 = (x_center - width / 2) * image.width\n",
    "y1 = (y_center - height / 2) * image.height\n",
    "x2 = (x_center + width / 2) * image.width\n",
    "y2 = (y_center + height / 2) * image.height\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "filtered_indices = scores > confidence_threshold\n",
    "x1, y1, x2, y2 = x1[filtered_indices], y1[filtered_indices], x2[filtered_indices], y2[filtered_indices]\n",
    "scores = scores[filtered_indices]\n",
    "\n",
    "def nms(boxes, scores, threshold):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        intersection = w * h\n",
    "        iou = intersection / (areas[i] + areas[order[1:]] - intersection)\n",
    "        keep_indices = np.where(iou <= threshold)[0]\n",
    "        order = order[keep_indices + 1]\n",
    "    return keep\n",
    "\n",
    "boxes_nms = np.stack([x1, y1, x2, y2], axis=1)\n",
    "keep_indices = nms(boxes_nms, scores, 0.5)\n",
    "boxes_nms = boxes_nms[keep_indices]\n",
    "scores_nms = scores[keep_indices]\n",
    "\n",
    "image_np = np.array(image)\n",
    "image_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "for i in range(len(boxes_nms)):\n",
    "    x1, y1, x2, y2 = boxes_nms[i]\n",
    "    cv2.rectangle(image_np, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "    cv2.putText(image_np, f\"Face {scores_nms[i]:.2f}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imwrite(\"tflite_image.jpg\", image_np)\n",
    "cv2.imshow(\"Result\", image_np)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf61dba-1149-4685-8ee7-ef3221a5b7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06d273a-e2e7-4c25-bbaa-b6c0f21979d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TinyTracker.tflite\", \"rb\") as f:\n",
    "    hex_data = f.read().hex()\n",
    "c_array = \"const unsigned char TinyTrackerS_tflite[] = {\" + \", \".join(f\"0x{hex_data[i:i+2]}\" for i in range(0, len(hex_data), 2)) + \"};\"\n",
    "with open(\"model_data.cc\", \"w\") as f:\n",
    "    f.write(c_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bf64cf-d2f2-4e6b-9cbe-62b1aa757b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TinyTrackerS.tflite\", \"rb\") as f:\n",
    "    model_data = f.read()\n",
    "\n",
    "hex_array = \", \".join(f\"0x{byte:02x}\" for byte in model_data)\n",
    "c_array = f\"\"\"#include <cstddef>\n",
    "alignas(16) const unsigned char TinyTrackerS_tflite[] = {{\n",
    "    {hex_array}\n",
    "}};\n",
    "const int TinyTrackerS_tflite_len = {len(model_data)};\n",
    "\"\"\"\n",
    "\n",
    "with open(\"model_data2.cc\", \"w\") as f:\n",
    "    f.write(c_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab42220-66c9-49b3-bde4-ed7d33acab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Ops: {'TANH', 'DEQUANTIZE', 'CONV_2D', 'DELEGATE', 'FULLY_CONNECTED', 'DEPTHWISE_CONV_2D', 'CONCATENATION', 'MEAN'}\n",
      "Total Ops Needed: 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.lite as tflite\n",
    "\n",
    "interpreter = tflite.Interpreter(model_path=\"TinyTrackerS.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "ops = set()\n",
    "for op in interpreter._get_ops_details():\n",
    "    ops.add(op[\"op_name\"])\n",
    "\n",
    "print(f\"Required Ops: {ops}\")\n",
    "print(f\"Total Ops Needed: {len(ops)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968d7e54-8b55-4fd6-b5a3-dfdfbfd99ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0:\n",
      "  Shape: [ 1 96 96  1]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Scale: 0.007048431318253279\n",
      "  Zero Point: -5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load your TFLite model\n",
    "model_path = \"TinyTrackerS.tflite\"  # Update with your actual model path\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Print input details\n",
    "for i, inp in enumerate(input_details):\n",
    "    print(f\"Input {i}:\")\n",
    "    print(f\"  Shape: {inp['shape']}\")\n",
    "    print(f\"  Data Type: {inp['dtype']}\")\n",
    "    print(f\"  Scale: {inp['quantization'][0]}\")   # Scale factor\n",
    "    print(f\"  Zero Point: {inp['quantization'][1]}\")  # Zero point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4035dbd-ded8-4062-9e55-ac738fff27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model expects grayscale images (1 channel).\n"
     ]
    }
   ],
   "source": [
    "# Check input shape\n",
    "input_shape = input_details[0]['shape']\n",
    "channels = input_shape[-1] if len(input_shape) == 4 else 1\n",
    "\n",
    "print(channels);\n",
    "if channels == 3:\n",
    "    print(\"Model expects RGB/BGR images (3 channels).\")\n",
    "elif channels == 1:\n",
    "    print(\"Model expects grayscale images (1 channel).\")\n",
    "else:\n",
    "    print(\"Unexpected input format:\", input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6329cb77-93b5-4760-803d-45d20c27c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 0:\n",
      "  Shape: [1 2]\n",
      "  Data Type: <class 'numpy.float32'>\n",
      "  Scale: 0.0\n",
      "  Zero Point: 0\n"
     ]
    }
   ],
   "source": [
    "for i, out in enumerate(output_details):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(f\"  Shape: {out['shape']}\")\n",
    "    print(f\"  Data Type: {out['dtype']}\")\n",
    "    print(f\"  Scale: {out['quantization'][0]}\")\n",
    "    print(f\"  Zero Point: {out['quantization'][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035d7bad-d29d-47f8-b7c0-ee2bffa1cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: [[-0.109375  0.140625]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a test input (black image, grayscale)\n",
    "test_input = np.zeros(input_details[0]['shape'], dtype=np.float32)  # Start as float32\n",
    "\n",
    "# Quantize the input: (Pixel - Zero Point) / Scale\n",
    "scale = input_details[0]['quantization'][0]\n",
    "zero_point = input_details[0]['quantization'][1]\n",
    "test_input = np.round(test_input / scale + zero_point).astype(np.int8)  # Convert to int8\n",
    "\n",
    "# Set input and run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get and print output\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Raw output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab7546-ab6c-4422-b9d0-9c192be38b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # Install with: pip install opencv-python\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "    img = cv2.resize(img, (96, 96))  # Resize to model's input size\n",
    "    img = img.astype(np.float32)  # Convert to float32\n",
    "\n",
    "    # Apply quantization (use the scale & zero point from the model)\n",
    "    scale = 0.007048\n",
    "    zero_point = -5\n",
    "    img = np.round(img / scale + zero_point).astype(np.int8)  # Quantize to int8\n",
    "\n",
    "    # Add batch dimension (1, 96, 96, 1)\n",
    "    img = np.expand_dims(img, axis=(0, -1))\n",
    "    return img\n",
    "\n",
    "# Example usage\n",
    "image_path = \"egp.jpg\"  # Replace with an actual test image\n",
    "processed_img = preprocess_image(image_path)\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], processed_img)\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Inference Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b3a65-8219-416b-91ee-30b57d7d997c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1b736-e8d3-4e8e-be9e-c32fa3103a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyserial pyautogui screeninfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf1bb1-5250-473c-aa0e-a6d72d6a0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import serial\n",
    "    import pyautogui\n",
    "    from screeninfo import get_monitors\n",
    "    print(\"✅ All packages loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Missing package: {e.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b492f-5519-4c67-9faa-2360db939188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d5231e-3f58-462f-8297-a5ddccd7852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen: 1920x1080\n",
      "Running gaze mouse... (Press any key or Ctrl+C to stop)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.39, 0.27)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 401ms | Loop: 406ms\n",
      "Output: Gaze: (0.40, 0.20)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.38, 0.20)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.39, 0.28)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.32, 0.24)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (-0.12, 0.14)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.24, 0.11)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (-0.11, 0.01)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (-0.37, 0.06)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (-0.09, 0.18)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (-0.18, 0.14)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (-0.30, 0.12)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.53, 0.09)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (-0.35, 0.06)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.22, 0.06)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 1ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.17, 0.06)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (0.20, 0.05)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 2ms | Inference: 401ms | Loop: 406ms\n",
      "Output: Gaze: (0.16, 0.06)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 16µs | Quant: 2ms | Inference: 402ms | Loop: 405ms\n",
      "Output: Gaze: (0.24, -0.03)\n",
      "Output: DRAM: 71468/323148 | PSRAM: 141524/4194304\n",
      "Output: FPS: 2.5 | Capture: 15µs | Quant: 1ms | Inference: 402ms | Loop: 406ms\n",
      "Output: Gaze: (0.20, -0.04)\n",
      "\n",
      "Key pressed. Stopping...\n",
      "Serial connection closed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import threading\n",
    "import serial\n",
    "import pyautogui\n",
    "import keyboard\n",
    "from screeninfo import get_monitors\n",
    "\n",
    "# Flag to control the loop\n",
    "running = True\n",
    "\n",
    "# Thread to listen for any key press\n",
    "def listen_for_keypress():\n",
    "    global running\n",
    "    keyboard.read_event()  # Wait for any key\n",
    "    print(\"\\nKey pressed. Stopping...\")\n",
    "    running = False\n",
    "\n",
    "# Start the key listening thread\n",
    "threading.Thread(target=listen_for_keypress, daemon=True).start()\n",
    "\n",
    "# Serial setup\n",
    "try:\n",
    "    ser = serial.Serial(\n",
    "        port='COM6',\n",
    "        baudrate=115200,\n",
    "        timeout=0.1\n",
    "    )\n",
    "except serial.SerialException as e:\n",
    "    print(f\"Serial error: {e}\")\n",
    "    print(\"Available ports:\")\n",
    "    import serial.tools.list_ports\n",
    "    print([p.device for p in serial.tools.list_ports.comports()])\n",
    "    sys.exit(1)\n",
    "\n",
    "screen = get_monitors()[0]\n",
    "print(f\"Screen: {screen.width}x{screen.height}\")\n",
    "\n",
    "def map_gaze(x, y):\n",
    "    return (\n",
    "        int((x + 1) * screen.width / 2),\n",
    "        int((1 - y) * screen.height / 2)\n",
    "    )\n",
    "\n",
    "print(\"Running gaze mouse... (Press any key or Ctrl+C to stop)\")\n",
    "\n",
    "try:\n",
    "    while running:\n",
    "        try:\n",
    "            line = ser.readline().decode('utf-8').strip()\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # skip bad data\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        print(f\"Output: {line}\")\n",
    "        if line.startswith(\"Gaze:\"):\n",
    "            try:\n",
    "                _, data = line.split(':')\n",
    "                data = data.strip().strip('()')\n",
    "                x, y = map(float, data.split(','))\n",
    "                mouse_x, mouse_y = map_gaze(x, y)\n",
    "                pyautogui.moveTo(mouse_x, mouse_y)\n",
    "            except ValueError:\n",
    "                print(f\"Parse error: {line}\")\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nCtrl+C detected. Stopping...\")\n",
    "    running = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    ser.close()\n",
    "    print(\"Serial connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d64f42-3b5f-4fac-99c5-30d9db8687af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d6e83-b959-4c29-8df0-26cf294decba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d78d5-a075-4860-8c45-d7e26f464793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0644d-5b41-4de5-95bf-623a45845069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42edd5-1fb3-4d24-9685-b201b62c300d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8423ed6-d739-4d00-aff7-3e60a36307b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b1505-de7d-43e7-ab04-6bd5c55f1847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269acdf6-8428-41c5-9b00-a11cce8fb2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d81dd9-71dc-49fa-a057-60dff7cd54a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44039b2-1dd8-483a-95f1-031ea1160a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff9f95-3de0-4b49-9cad-e65bd04e5337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9964d-1643-4e69-9f1f-d42bef87ebe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c96bff-f0a9-4dd3-9c0c-076c896421db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5723c42-806c-4453-ad23-9098bd65d89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777eb81-87de-44ac-a9ac-213faf50a62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1c8662-1b5b-4a60-a76e-54b80e8ee296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'q' pressed, stopping program...\n"
     ]
    }
   ],
   "source": [
    "import keyboard\n",
    "import os\n",
    "import time\n",
    "\n",
    "def open_virtual_keyboard():\n",
    "    os.system(\"osk.exe\")\n",
    "\n",
    "\n",
    "keyboard_opened = False \n",
    "while True:\n",
    "    if keyboard.is_pressed('a') and not keyboard_opened:\n",
    "        print(\"Key 'a' pressed, opening virtual keyboard...\")\n",
    "        open_virtual_keyboard()\n",
    "        time.sleep(1)\n",
    "\n",
    "    if keyboard.is_pressed('q'):\n",
    "        print(\"Key 'q' pressed, stopping program...\")\n",
    "        break\n",
    "\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f7699-addf-42c9-b971-d7e778f96ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a3236-02df-4a8c-b183-f44a1f3e1d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b740ef4-fff9-4fd7-a6ce-4f730faeb672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee788f4-8856-4f7c-b062-ff4b0be98759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c52ea1-605a-4c34-8dc2-cf893bd62c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
